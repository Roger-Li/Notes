{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [notebook](https://github.com/ageron/handson-ml/blob/master/06_decision_trees.ipynb) of detailed implementation of chapter by [Aur√©lien Geron](https://github.com/ageron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "* Decision Trees require very little data preparation. Don't require feature scaling or centering.\n",
    "* Gini impurity $G_i = \\sum_{k=1}^np_k\\sum_{k'\\neq k}p_{k'} = \\sum_{k=1}^n p_k(1-p_k) = 1 - \\sum_{k=1}^n p_{k}^2$, where $p_{k}$ is the ratio of class $k$ ($=1,...,n$)instances among the training instances in the $i^{th}$ node.\n",
    "* Gini index $=0$ means true purity.\n",
    "* Scikit-Learn uses the CART algorithm, which produces only binary trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Class Probabilities\n",
    "* DT can estimate the probability that an instance belongs to a particular class $k$, just using the ratio of training instances of class $k$ in the leaf node $i$ (i.e., $p_{i,k}$). **Notice** that as long as an instance belongs to a particular leaf node, changing its features won't change its probability estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CART Algorithm\n",
    "* CART split the set at a node into two subsets using a single feature $k$ and a threshold $t_k$.\n",
    "* It searches for the pair $(k,t_k)$ that produces the purest subsets weighted by size.\n",
    "* CART cost function (minimization)\n",
    "$$J(k,t_k)=\\dfrac{m_{left}}{m}G_{left}+\\dfrac{m_{right}}{m}G_{right}$$\n",
    "Where $m$ is the number of instances in each subsets, $G_{\\cdot}$ measures the impurity of the subset.\n",
    "* CART stops when reaching **max_depth** or, cannot find a split that reduces impurity, or other hit additional stopping conditions.\n",
    "* CART is a *greedy algorithm*, not guarenteed to find the optimal solution.\n",
    "* Finding the optimal tree is *NP-Complete*, and requires $O(\\exp(m))$ time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexity\n",
    "* Making predictions $=$ traversing the DT requries $O(\\log_2(m))$\n",
    "* CART only checks one feature to split at each node, so the overall prediction complexity is still $O(\\log_2(m))$\n",
    "* For training, it compares all features on all samples at each node, thus requires $O(n\\times m\\log(m))$. ($m$ samples with $n$ features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini Impurity VS Entropy\n",
    "* $H_i = \\underset{p_{i,k}\\neq0}{-\\sum_{k=1}^n}p_{i,k}\\cdot \\log(p_{i,k})$ \n",
    "* Entropy is zero when a node contains instances of only one class.\n",
    "* In practice, Gini impurity tends to isolate the most frequent class in its own branch of the tree while entropy tends to produce slightly more balanced trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization Hyperparameters\n",
    "* DT is *non-parametric model* because the number of parameters is not determined prior to training.\n",
    "* Other algorithms work by first training the DT unrestricted, then *pruning* unnecessary nodes. Use $\\chi^2$ test to determine the statistical significance of the improvement of a node whose children are all leaf nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "* $$J(k,t_k)=\\dfrac{m_{left}}{m}\\text{MSE}_{left}+\\dfrac{m_{right}}{m}\\text{MSE}_{right}$$ where $$\\text{MSE}_{node} = \\sum_{i \\in node}(\\hat{y}_{node}-y^{(i)})^2$$ and $$\\hat{y}_{node}=\\dfrac{1}{m_{node}}\\sum_{i \\in node}y^(i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instability\n",
    "* DT generates orthogonal decision boundaries, which makes them sensitive to training set rotation. This problem could be limited by using PCA.\n",
    "* Sensitive to small variations in the training data. This problem could be limited by using Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "[A Practical Guide to Tree Based Learning Algorithms](https://sadanand-singh.github.io/posts/treebasedmodels/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}