{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifiers\n",
    "* *Hard voting:* majority-voting classifier\n",
    "* *Soft voting* predicts the class with the highest probability averaged over all the individual classifiers. It often achieved higher performance than hard voting because it gives more weight to highly confident votes.\n",
    "* For classifiers that does not generate probability such as SVC, check [Platt Scaling] (https://en.wikipedia.org/wiki/Platt_scaling). Basically it snaps a logistic function on top of the SVM decision function, and calibrate the parameters using cross-validation.\n",
    "* **Why does ensembling work?** *Law of large numbers*. think of tossing a biased coin many times, the more you tosse it, the more likely you are going to find the head. Similarly, ensemble of *weak leaners* can still be a *strong learner (achieving high accuracy)* provided there are a sufficient number of weak learners and they are sufficiently diverse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and Pasting\n",
    "* *Bagging*: sampling *with* replacement. *Pasting*: *without* replacement.\n",
    "* [Bagging VS Pasting](https://stats.stackexchange.com/questions/219193/when-should-the-pasting-ensemble-method-be-used-instead-of-bagging), basically when size of the dataset is small, bagging is always the choice, pasting might be preferrable with large sample size and on external validations.\n",
    "* In Scikit-Learn, it is selected by the hyperparameter **bootstrap** (*=True* for bagging, *=False* for pasting.)\n",
    "\n",
    "### Out-of-Bag Evaluation\n",
    "* in Scikit-Learn, set **oob_score=True** to request automatic oob evaluations after training, which could be used as an estimate for testing set performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "* Sampling both training instances and features is called *Random Patches*; keeping all training instances but sampling features is called *Random Subspaces*.\n",
    "* RF searches for the best feature among a random subset of features when growing trees in order to introduce extra randomness. It trades a higher bias for a lower variance, generally yielding an overall better model. \n",
    "\n",
    "### Extra-Trees\n",
    "* On top of RF, also using random thresholds for each feature rather than searching for the best possible thresholds like regular Decision Trees do. Called *Extremely Randomized Trees* ensemble.\n",
    "* Much faster to train.\n",
    "\n",
    "### Feature Importance\n",
    "* Important features are likely to appear closer to the root, so it is possible to estimate a feature's importance by computing the average depth at which it appears across all trees in the forest.\n",
    "* [Gini importance](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#giniimp): \"Every time a split of a node is made on feature $m$ the gini impurity criterion for the two children nodes is less than the parent node. Adding up the gini decreases for each individual feature over all trees in the forest gives a fast feature importance that is often very consistent with the permutation importance measure.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "### AdaBoost\n",
    "\n",
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "* Detailed [implementation](https://github.com/ageron/handson-ml/blob/master/07_ensemble_learning_and_random_forests.ipynb) of this chapter by [Aur√©lien Geron](https://github.com/ageron)\n",
    "* [Bagging VS Pasting](https://stats.stackexchange.com/questions/219193/when-should-the-pasting-ensemble-method-be-used-instead-of-bagging)\n",
    "* [Gini importance](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#giniimp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
