{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifiers\n",
    "* *Hard voting:* majority-voting classifier\n",
    "* *Soft voting* predicts the class with the highest probability averaged over all the individual classifiers. It often achieved higher performance than hard voting because it gives more weight to highly confident votes.\n",
    "* For classifiers that does not generate probability such as SVC, check [Platt Scaling] (https://en.wikipedia.org/wiki/Platt_scaling). Basically it snaps a logistic function on top of the SVM decision function, and calibrate the parameters using cross-validation.\n",
    "* **Why does ensembling work?** *Law of large numbers*. think of tossing a biased coin many times, the more you tosse it, the more likely you are going to find the head. Similarly, ensemble of *weak leaners* can still be a *strong learner (achieving high accuracy)* provided there are a sufficient number of weak learners and they are sufficiently diverse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and Pasting\n",
    "* *Bagging*: sampling *with* replacement. *Pasting*: *without* replacement.\n",
    "* [Bagging VS Pasting](https://stats.stackexchange.com/questions/219193/when-should-the-pasting-ensemble-method-be-used-instead-of-bagging), basically when size of the dataset is small, bagging is always the choice, pasting might be preferrable with large sample size and on external validations.\n",
    "* In Scikit-Learn, it is selected by the hyperparameter **bootstrap** (*=True* for bagging, *=False* for pasting.)\n",
    "\n",
    "### Out-of-Bag Evaluation\n",
    "* in Scikit-Learn, set **oob_score=True** to request automatic oob evaluations after training, which could be used as an estimate for testing set performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "* Sampling both training instances and features is called *Random Patches*; keeping all training instances but sampling features is called *Random Subspaces*.\n",
    "* RF searches for the best feature among a random subset of features when growing trees in order to introduce extra randomness. It trades a higher bias for a lower variance, generally yielding an overall better model. \n",
    "\n",
    "### Extra-Trees\n",
    "* On top of RF, also using random thresholds for each feature rather than searching for the best possible thresholds like regular Decision Trees do. Called *Extremely Randomized Trees* ensemble.\n",
    "* Much faster to train.\n",
    "\n",
    "### Feature Importance\n",
    "* Important features are likely to appear closer to the root, so it is possible to estimate a feature's importance by computing the average depth at which it appears across all trees in the forest.\n",
    "* [Gini importance](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#giniimp): \"Every time a split of a node is made on feature $n$ the gini impurity criterion for the two children nodes is less than the parent node. Adding up the gini decreases for each individual feature over all trees in the forest gives a fast feature importance that is often very consistent with the permutation importance measure.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "* *Sequentially* train predictors to correct its predecessors.\n",
    "\n",
    "### AdaBoost\n",
    "Initiate each instance weight $w^{(i)}$ to $\\frac{1}{m}$\n",
    "* Calculate *weighted error rate* of the $j^{\\text{th}}$ predictor\n",
    "$$r_j = \\dfrac{\\underset{\\hat{y}_j^{(i)}\\neq y^{(i)}}{\\sum_{i=1}^m}w^{(i)}}{\\sum_{i=1}^mw^{(i)}}$$\n",
    "* Compute predictor weight\n",
    "$$\\alpha_j=\\eta\\log\\dfrac{1-r_j}{r_j}$$\n",
    "Note that if a predictor is just guessing randomly ($r_j\\approx0.5$), then tis weight will be close to zero.\n",
    "* Update instance weight, for  $i=1,...,m$\n",
    "$$w^{(i)}=\\left\\{\n",
    "                \\begin{array}{ll}\n",
    "                  w^{(i)}, \\qquad \\qquad \\hat{y}_j^{(i)}= y^{(i)}\\\\\n",
    "                  w^{(i)}\\exp(\\alpha_j), \\quad \\hat{y}_j^{(i)}\\neq y^{(i)}\n",
    "                \\end{array}\n",
    "              \\right.$$\n",
    "and then normalize all instance weights (i.e., divided by $\\sum_{i=1}^m w^{(i)}$). Note that if $r_j = 0$ (worst possible prediction), then $\\alpha_j=\\infty$, and $w^{(i)}$ gets very big for miss-classified cases.\n",
    "* Finally, AdaBoost precitions\n",
    "$\\hat{y}(x) =\\underset{k}{\\text{argmax}}\\underset{\\hat{y}_j(x) = k}{\\sum_{j=1}^N}\\alpha_j$\n",
    "* If AdaBoost ensemble is overfitting the training set, reduce the # of estimators or more strongly regularize the base estimator.\n",
    "\n",
    "\n",
    "### Gradient Boosting\n",
    "* Instead of tweaking the instance weights at every iteration, [Gradient Boosting](https://en.wikipedia.org/wiki/Gradient_boosting#Informal_introduction) tries to fit the new predictor to the *residual error*, **why**? \n",
    "\n",
    "\"A generalization of this idea to loss functions other than squared error - and to classification and ranking problems - follows from the observation that residuals $y - F(x)$ for a given model are the negative gradients (with respect to $F(x)$) of the squared error loss function $\\frac{1}{2}(y - F(x))^2$. So, gradient boosting is a gradient descent algorithm; and generalizing it entails \"plugging in\" a different loss and its gradient.\"\n",
    "\n",
    "Classification or ranking models could be integrated with the \"gradient descent\" setting by applying decision function or ranking score functions.\n",
    "* The *Stochastic Gradient Boosting* uses a subsample of instances for training each tree.\n",
    "* The learning_rate hyperparameter scales the contribution of each tree, regularize this hyperparameter is called *shrinkage*.\n",
    "* To find the optimal # of trees, use early stopping\n",
    "\n",
    "#### Note that although Boosting often works better for lowering biases, it's not parallelizable like bagging algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "* Train *meta learners* to take predictions from different predictors as inputs and makes the final prediction.\n",
    "* Could be multi-layer stacking ensemble, as long as the meta learner is trained using a hold-out set (or out-of-fold predictions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "* Detailed [implementation](https://github.com/ageron/handson-ml/blob/master/07_ensemble_learning_and_random_forests.ipynb) of this chapter by [Aur√©lien Geron](https://github.com/ageron)\n",
    "* [Bagging VS Pasting](https://stats.stackexchange.com/questions/219193/when-should-the-pasting-ensemble-method-be-used-instead-of-bagging)\n",
    "* [Gini importance](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#giniimp)\n",
    "* Previous [notes](https://github.com/Roger-Li/Notes/blob/master/note_cmu_ml/10_boosting.pdf) of step-by-step demonstration of AdaBoost."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
