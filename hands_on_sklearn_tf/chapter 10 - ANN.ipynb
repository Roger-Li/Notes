{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The perceptron\n",
    "* Common step functions used in Perceptrons: $\\text{heaviside}(z)$, $\\text{sgn}(z)$\n",
    "* Perceptron learning rule $w_{i,j}^{(\\text{next step})} = w_{i,j}+\\eta(\\hat{y}_j-y_j)x_j$. This will guarentee to converge to one of the solutions for linear separable data (Note the separating hyperplane is often not unique)\n",
    "* One-layer perceptrons are incapable of learning complex patterns (like Logistic Regression). However, Multi-Layer Perceptrons (MLP) can solve complex problems such as XOR.\n",
    "* Because perceptrons use step functions for activation, back-propagation (DB) does not work. Has to change the step functions to something like a logistic function, ReLU, hyperbolic tangent, etc\n",
    "* For multi-label classification problems, add a shared $softmax$ function at the output layer.\n",
    "\n",
    "#### Perceptron Vs Logistic Regression\n",
    "* (One-layer) Perceptron will converge only if the dataset is linearly separable, and won't be able to estimate class probabilities. Neither of which are true for Logistic Regression. \n",
    "* Replacing the activation function to logistic/softmax and using Gradient Descent will make perceptron equivalent to a Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning Neural Network Hyperparameters\n",
    "### Number of Hidden Layers\n",
    "* Deep networks have a much higher *parameter efficiency* than shallow ones, uses exponentially fewer neurons, making them faster to train.\n",
    "* Lower hidden layers model low-level structures (e.e., line segments of various shapes andorientations); intermediate hidden layers combine these low-level structures to model intermediate-level structures (e.g., squares, circles); the highest hidden layers and the output layer combine these intermediate structures to model high=level structures (e.g., faces)\n",
    "* DNN converge faster to a good solution, and also improves ability to generalize to new datasets.\n",
    "* more common to reuse parts of a pretrained state-of-the-art network that performs a similar task.\n",
    "\n",
    "### Number of Neurons per Hidden Layer\n",
    "* Previous common practice is to size them to form a funnel with fewer and fewer neurons at each layer.\n",
    "* Now may simply use the same size for all hidden layers (just one hyper-parameter to tune) + early-stopping.\n",
    "* Will get more bang for the buck by increasing the number of layers than the number of neurons per layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "* Full implementation ([Jupyter Notebook](https://github.com/ageron/handson-ml/blob/master/10_introduction_to_artificial_neural_networks.ipynb)) by [Aur√©lien Geron](https://github.com/ageron)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
