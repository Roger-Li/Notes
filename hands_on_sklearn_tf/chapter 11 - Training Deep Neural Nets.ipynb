{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing/Explding Gradients Problems\n",
    "\n",
    "* *Vanishing gradients* problem: Gradients often get smaller and smaller as the algorithm progresses down to the lower layers. Thus, the Gradient Descent update leaves he lower layer connection weights virtually unchanged so training never converges to a good solution.\n",
    "\n",
    "* *Exploding gradients* problem is the opposite, gradietns grow bigger and bigger that somany layers get insanely large weight updates and the algorithm diverges. Mostly encountered in recurrent neural netowrks.\n",
    "\n",
    "Gradient problems are worse with logistic activation functions, as the function saturates at $0$ or $1$ (gradients $\\rightarrow 0$)\n",
    "\n",
    "### Xavier and He Initialization\n",
    "For the signal to flow through the layers properly, we need the variance of the outputs of each layer to be equal to the variance of its inputs, also the gradients to have equal variance before and after flowing through a layer in the reverse directions. Not possible unless the layer has equal number of input and output connections, but some practices have been proven to work well.\n",
    "\n",
    "\n",
    "| Activation function        | Uniform distribution [-r,r]           | Normal distribution  |\n",
    "| :-------------|:-------------:| :-------------:|\n",
    "| Logistic      | $r = \\sqrt{\\dfrac{6}{n_{inputs}+n_{outputs}}}$ | $\\sigma = \\sqrt{\\dfrac{2}{n_{inputs}+n_{outputs}}}$ |\n",
    "| Hyperbolic tangent      | $r = 4\\sqrt{\\dfrac{6}{n_{inputs}+n_{outputs}}}$ | $\\sigma = 4\\sqrt{\\dfrac{2}{n_{inputs}+n_{outputs}}}$|\n",
    "| ReLu (variants) |  $r = \\sqrt{2}\\sqrt{\\dfrac{6}{n_{inputs}+n_{outputs}}}$ | $\\sigma = \\sqrt{2}\\sqrt{\\dfrac{2}{n_{inputs}+n_{outputs}}}$ |\n",
    "\n",
    "\n",
    "### Nonsaturating Activation Function\n",
    "* ReLU suffers from *dying*. ifa neuron's weights get updated such that the weighted sum of the neuron's inputs is negative, it will start outputting $0$ and is unlikely to come back to life since the gradient of ReLU is 0 when having negative input.\n",
    "* (pp. 279 - 280) *leaky* ReLU, *randomized leaky* ReLU (RReLU), *parametric leaky* ReLU (PReLU), *exponential linear unit* (ELU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ReLU(x) =\\left \\{\\begin{array}{ll}  \n",
    "                             0 & x<0 \\\\ \n",
    "                             x & x\\geq 0 \n",
    "                   \\end{array} \\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Gradient Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing Pretrained Layers\n",
    "\n",
    "### Reusing a TensorFlow Model\n",
    "\n",
    "### REusing Models from Other Frameworks\n",
    "\n",
    "### Freezing the Lower Layers\n",
    "\n",
    "### Cachine the Frozen Layers\n",
    "\n",
    "### Tweaking, Dropping, or Replacing the Upper Layers\n",
    "\n",
    "### Model Zoos\n",
    "\n",
    "### Unsupervised Pretraining\n",
    "\n",
    "### Pretraining on an Auxiliary Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "* Full [implementation](https://github.com/ageron/handson-ml/blob/master/11_deep_learning.ipynb) By [AurÃ©lien Geron](https://github.com/ageron)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
