{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing/Explding Gradients Problems\n",
    "\n",
    "* *Vanishing gradients* problem: Gradients often get smaller and smaller as the algorithm progresses down to the lower layers. Thus, the Gradient Descent update leaves he lower layer connection weights virtually unchanged so training never converges to a good solution.\n",
    "\n",
    "* *Exploding gradients* problem is the opposite, gradietns grow bigger and bigger that somany layers get insanely large weight updates and the algorithm diverges. Mostly encountered in recurrent neural netowrks.\n",
    "\n",
    "Gradient problems are worse with logistic activation functions, as the function saturates at $0$ or $1$ (gradients $\\rightarrow 0$)\n",
    "\n",
    "### Xavier and He Initialization\n",
    "For the signal to flow through the layers properly, we need the variance of the outputs of each layer to be equal to the variance of its inputs, also the gradients to have equal variance before and after flowing through a layer in the reverse directions. Not possible unless the layer has equal number of input and output connections, but some practices have been proven to work well.\n",
    "\n",
    "\n",
    "| Activation function        | Uniform distribution [-r,r]           | Normal distribution  |\n",
    "| :-------------|:-------------:| :-------------:|\n",
    "| Logistic      | $r = \\sqrt{\\dfrac{6}{n_{inputs}+n_{outputs}}}$ | $\\sigma = \\sqrt{\\dfrac{2}{n_{inputs}+n_{outputs}}}$ |\n",
    "| Hyperbolic tangent      | $r = 4\\sqrt{\\dfrac{6}{n_{inputs}+n_{outputs}}}$ | $\\sigma = 4\\sqrt{\\dfrac{2}{n_{inputs}+n_{outputs}}}$|\n",
    "| ReLu (variants) |  $r = \\sqrt{2}\\sqrt{\\dfrac{6}{n_{inputs}+n_{outputs}}}$ | $\\sigma = \\sqrt{2}\\sqrt{\\dfrac{2}{n_{inputs}+n_{outputs}}}$ |\n",
    "\n",
    "\n",
    "### Nonsaturating Activation Function\n",
    "* ReLU suffers from *dying*. ifa neuron's weights get updated such that the weighted sum of the neuron's inputs is negative, it will start outputting $0$ and is unlikely to come back to life since the gradient of ReLU is 0 when having negative input.\n",
    "* (pp. 279 - 280) *leaky* ReLU, *randomized leaky* ReLU (RReLU), *parametric leaky* ReLU (PReLU), *scaled exponential linear unit* (SELU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $ReLU(xz) = \\max (0, z)$\n",
    "* $LeakyReLU_\\alpha(x) = \\max (\\alpha z, z)$\n",
    "<img src=\"images\\11_leaky_relu_plot.png\" width = \"300\" alt=\"Leaky Relu\">\n",
    "* $ELU(z)_\\alpha =\\left \\{\\begin{array}{ll}  \n",
    "                             \\alpha (\\exp (z)-1) & z<0 \\\\ \n",
    "                             z & z\\geq 0 \n",
    "                   \\end{array} \\right.$\n",
    "<img src=\"images\\11_elu_plot.png\" width = \"300\" alt=\"ELU\">\n",
    "\n",
    "* $SELU(z)_\\alpha = \\lambda \\left \\{\\begin{array}{ll}  \n",
    "                             \\alpha (\\exp (z)-1) & x<0 \\\\ \n",
    "                             z & z\\geq 0 \n",
    "                   \\end{array} \\right.$\n",
    "<img src=\"images\\11_selu_plot.png\" width = \"300\" alt=\"SELU\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "\n",
    "* At training time, use the mean and variance of the current batch to center (-$\\mu$) and normalize (/$\\sigma$) each input, and then apply scaling ($\\cdot \\gamma$) and shifting ($+\\beta$) \n",
    "\n",
    "* At test time, use the empirical mean and standard deviation of the whole training set, note that $\\mu, \\sigma, \\gamma, \\beta$ are learned for each batch-normalized layer.\n",
    "\n",
    "### BN with TensorFlow\n",
    "* (pp. 284 - 285)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping (pp 286)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing Pretrained Layers\n",
    "* Generally, transfer learning will work only well if the inputs have similar low-level features.\n",
    "\n",
    "### Reusing a TensorFlow Model (pp.287)\n",
    "\n",
    "### Reusing Models from Other Frameworks (pp.288)\n",
    "\n",
    "### Freezing the Lower Layers (pp. 289)\n",
    "* `optimizer.minimize(loss, var_list = ...)`\n",
    "\n",
    "### Cachine the Frozen Layers (pp.290)\n",
    "* Run the whole training set through the frozen lower layers once, and then use mini-batches of the outputs to train higher layers.\n",
    "\n",
    "### Tweaking, Dropping, or Replacing the Upper Layers (pp. 290)\n",
    "* Find the right number of layers to reuse\n",
    "\n",
    "### Model Zoos (pp.291)\n",
    "\n",
    "### Unsupervised Pretraining (pp.291)\n",
    " * If we have a complex task but not much labeled training data, we can use unsupervised pretraining (e.g., auto-encoders) to determine the network structure, and then train the model using the labeled data.\n",
    "\n",
    "### Pretraining on an Auxiliary Task (pp.292)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "* Full [implementation](https://github.com/ageron/handson-ml/blob/master/11_deep_learning.ipynb) By [Aur√©lien Geron](https://github.com/ageron)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
